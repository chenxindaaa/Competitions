{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import gc\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_colwidth = 200\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log message\n",
    "\n",
    "sel_data = pd.read_csv('../data/preliminary_sel_log_all.csv') #x训练的日志文件\n",
    "sel_data2 = pd.read_csv('../tcdata/final_sel_log_dataset_b.csv')  # 提交的日志文件\n",
    "sel_data = pd.concat([sel_data, sel_data2])\n",
    "sel_data['time'] = pd.to_datetime(sel_data['time'])\n",
    "sel_data.sort_values(by=['sn', 'time'], inplace=True)\n",
    "sel_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "\n",
    "train_data = pd.read_csv(\"../data/preliminary_train_label_dataset_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional training set\n",
    "\n",
    "train_data_a = pd.read_csv(\"../data/preliminary_crashdump_dataset.csv\")\n",
    "train_data_a = pd.concat((train_data_a, pd.read_csv('../tcdata/final_crashdump_dataset_b.csv')))\n",
    "train_data_a['label'] = train_data_a['fault_code'].apply(lambda x:0 if x.split('.')[0] == 'cpu0' else 1)\n",
    "train_data = pd.concat((train_data, train_data_a.loc[:, ['sn', 'fault_time', 'label']]))\n",
    "train_data = train_data.drop_duplicates()\n",
    "train_data['fault_time'] = pd.to_datetime(train_data['fault_time'])\n",
    "train_data.sort_values(by=['sn', 'fault_time'], inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../tcdata/final_submit_dataset_b.csv').loc[:, ['sn', 'fault_time']]\n",
    "test_data['fault_time'] = pd.to_datetime(test_data['fault_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1697746it [28:08, 1005.72it/s] \n"
     ]
    }
   ],
   "source": [
    "# 20分钟\n",
    "sel_data = sel_data.copy()    # 制作特征词向量\n",
    "for name,row in tqdm(sel_data.iterrows()):\n",
    "    s = row[\"msg\"]\n",
    "    msg_lsit =s.split()  \n",
    "    if \"OEM record ef\" in s:  # 删除含有 OME record efd 列\n",
    "        sel_data.drop(index=[name],inplace=True)   \n",
    "    if msg_lsit[0].lower() == \"unknown\" and re.search(\"#0.+\",msg_lsit[1].lower()):\n",
    "        sel_data.drop(index=[name],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstFilter(ele):\n",
    "    s = re.sub('(#0x..\\s)|(/\\s)','',ele)\n",
    "    s = re.sub('cpu.*_','cpu_',s)\n",
    "    s = re.sub('(\\|\\s)','',s)\n",
    "    s = re.sub('(\\d+)','',s)\n",
    "    s = re.sub('(_{2,})','_',s)\n",
    "    s = re.sub('锛�','', s)\n",
    "    s = re.sub('锟絋',' ', s)\n",
    "    s = re.sub('锟�','', s)\n",
    "    # s = re.sub('\\|',',', s)\n",
    "    # s = re.sub('s4/s5:', 'ss_one_state', s)\n",
    "    # s = re.sub('s0/g0:', 'sg_one_state', s)\n",
    "    # s = re.sub('s5/g2:', 'sg_two_state', s)\n",
    "    s = re.sub('s4/s5:', 'ss', s)\n",
    "    s = re.sub('s0/g0:', 'sg', s)\n",
    "    s = re.sub('s5/g2:', 'sg', s)\n",
    "    s = re.sub('aa17.{22}','', s)\n",
    "    # s = re.sub('000000','a Special tags', s)\n",
    "    s = re.sub('\\d{4}\\w\\d\\w\\d{5}','Asserted oem record', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sel_data.groupby(['sn'], as_index=False)['msg'].agg(list)\n",
    "tmp['text'] = tmp['msg'].apply(lambda x: (\"\\n\".join([i for i in x])).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['text_1'] = tmp['text'].apply(FirstFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list = tmp['text_1'].values.tolist()\n",
    "sentences = list()\n",
    "for s in sentences_list:\n",
    "    sentences.append([w for w in s.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences, vector_size=32, window=3, min_count=5, sg=0, hs=1, seed=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_mean(sentences):\n",
    "    emb_matrix = list()\n",
    "    vec = list()\n",
    "    for w in sentences.split():\n",
    "        if w in w2v_model.wv:\n",
    "            vec.append(w2v_model.wv[w])\n",
    "    if len(vec) > 0:\n",
    "        emb_matrix.append(np.mean(vec, axis=0))\n",
    "    else:\n",
    "        emb_matrix.append([0] * w2v_model.vector_size)\n",
    "    return emb_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=50000, min_df=5, ngram_range=(1, 3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = list(tmp['text'].values)\n",
    "tfv = TfidfVectorizer(ngram_range=(1,3), min_df=5, max_features=50000)\n",
    "tfv.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf = tfv.transform(X)\n",
    "svd = TruncatedSVD(n_components=16)\n",
    "svd.fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_svd(sentences, n_components=16):\n",
    "    X_tfidf = tfv.transform(sentences)\n",
    "    X_svd = svd.transform(X_tfidf)\n",
    "    return np.mean(X_svd, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_data['time_ts'] = sel_data[\"time\"].values.astype(np.int64) // 10 ** 9\n",
    "train_data['fault_time_ts'] = train_data[\"fault_time\"].values.astype(np.int64) // 10 ** 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_split(strs, n, sep='|'):\n",
    "    str_li = strs.split(sep)\n",
    "    if len(str_li) >= n + 1:\n",
    "        return str_li[n]\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "sel_data['msg_split_0'] = sel_data['msg'].apply(lambda x: safe_split(x, 0))\n",
    "sel_data['msg_split_1'] = sel_data['msg'].apply(lambda x: safe_split(x, 1))\n",
    "sel_data['msg_split_2'] = sel_data['msg'].apply(lambda x: safe_split(x, 2))\n",
    "sel_data['category'] = sel_data['msg'].apply(lambda x: x.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_map = {\n",
    "    'Memory': 0,\n",
    "    'System': 1,\n",
    "    'Processor': 2,\n",
    "    'Temperature': 3,\n",
    "    'Drive': 4,\n",
    "    'Power': 5,\n",
    "    'Unknown': 6,\n",
    "    'Microcontroller': 7,\n",
    "    'OS': 8,\n",
    "    'Watchdog2': 9,\n",
    "    'OEM': 10,\n",
    "    'Button': 11,\n",
    "    'Slot/Connector': 12,\n",
    "    'Microcontroller/Coprocessor': 13,\n",
    "    'Management': 14,\n",
    "    'Event': 15,\n",
    "    'Watchdog': 16,\n",
    "    'Slot': 17,\n",
    "    'Fan': 18,\n",
    "    'Critical': 19,\n",
    "    'device': 20,\n",
    "    'LAN': 21,\n",
    "    'Version': 22,\n",
    "    'Add-in': 23,\n",
    "    'Terminator': 24,\n",
    "    'Chassis': 25,\n",
    "    'reserved': 26,\n",
    "    'Physical': 27,\n",
    "    'Session': 28,\n",
    "    'Reserved': 29,\n",
    "    'Cable/Interconnect': 30,\n",
    "    'Cable': 31,\n",
    "    'Chip': 32,\n",
    "    'Battery': 33\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 34/34 [00:39<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(cate_map):\n",
    "    sel_data[f'{i}_counts'] = sel_data['msg'].apply(lambda x: len(re.findall(i, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dataset, data_type='train'):\n",
    "    ret = list()\n",
    "\n",
    "    for idx, row in tqdm(dataset.iterrows()):\n",
    "        sn = row['sn']\n",
    "        fault_time = row['fault_time']\n",
    "        \n",
    "        # ts = row['fault_time_ts']\n",
    "        # 选取数据\n",
    "        days = 1  # 一天\n",
    "        hours = 1\n",
    "        tail_nums = 40   #数据量\n",
    "        LimitedTime = fault_time- pd.Timedelta(days=days)  # 计算前一天的日期\n",
    "        LimitedTime2 = fault_time + pd.Timedelta(hours=hours)  # 计算后一小时的日期\n",
    "        df = sel_data[(sel_data['sn'] == sn)&(sel_data[\"time\"] > LimitedTime)&(sel_data[\"time\"] < LimitedTime2)].copy()\n",
    "\n",
    "        if data_type == 'train':\n",
    "            label = row['label']\n",
    "\n",
    "        # df = sel_data[sel_data['sn'] == sn].copy()\n",
    "        # df = df[df['time_ts'] <= ts].copy()\n",
    "        df = df.sort_values(by='time_ts').reset_index(drop=True)\n",
    "        df = df.tail(tail_nums).copy()        # TODO: could change last 40 logs here\n",
    "\n",
    "        # make some features\n",
    "\n",
    "        logs_count = len(df)\n",
    "\n",
    "        if logs_count > 0:\n",
    "            msg_nunique = df['msg'].nunique()   # 统计不同的量\n",
    "            msg_category_nunique = df['category'].nunique()\n",
    "            msg_split_0_nunique = df['msg_split_0'].nunique()\n",
    "            msg_split_1_nunique = df['msg_split_1'].nunique()\n",
    "            msg_split_2_nunique = df['msg_split_2'].nunique()\n",
    "            last_category = df['category'].value_counts().index[0]\n",
    "            last_category = cate_map[last_category] if last_category in cate_map else len(cate_map)\n",
    "            s = df['time_ts'].values\n",
    "            if len(s) > 0:\n",
    "                seconds_span = s[-1] - s[0] \n",
    "            else:\n",
    "                seconds_span = 0\n",
    "\n",
    "            df['time_ts_shift_1'] = df['time_ts'].shift(1)\n",
    "            df['time_ts_diffs_1'] = df['time_ts'] - df['time_ts_shift_1']\n",
    "            s = df['time_ts_diffs_1'].values\n",
    "            if len(s) > 1:\n",
    "                log_time_diffs_avg = np.mean(s[1:])\n",
    "                log_time_diffs_max = np.max(s[1:])\n",
    "                log_time_diffs_min = np.min(s[1:])\n",
    "                log_time_diffs_std = np.std(s[1:])\n",
    "            else:\n",
    "                try:\n",
    "                    log_time_diffs_avg = log_time_diffs_max = log_time_diffs_min = s[0]\n",
    "                    log_time_diffs_std = 0\n",
    "                except:\n",
    "                    log_time_diffs_avg = log_time_diffs_max = log_time_diffs_min = log_time_diffs_std = 0\n",
    "\n",
    "            all_msg = \"\\n\".join(df['msg'].values.tolist()).lower()\n",
    "            new_msg = FirstFilter(all_msg)\n",
    "            w2v_emb = get_w2v_mean(new_msg)[0]\n",
    "            tfv_emb = get_tfidf_svd([s.lower() for s in df['msg'].values.tolist()])\n",
    "\n",
    "        else:\n",
    "#             continue\n",
    "            logs_count = 0\n",
    "            msg_nunique = 0\n",
    "            msg_category_nunique = 0\n",
    "            msg_split_0_nunique = 0\n",
    "            msg_split_1_nunique = 0\n",
    "            msg_split_2_nunique = 0\n",
    "            last_category = 0\n",
    "            seconds_span = 0\n",
    "            log_time_diffs_avg = 0\n",
    "            log_time_diffs_max = 0\n",
    "            log_time_diffs_min = 0\n",
    "            log_time_diffs_std = 0\n",
    "            w2v_emb = [0] * 32\n",
    "            tfv_emb = [0] * 16\n",
    "\n",
    "\n",
    "        # format dataset\n",
    "        data = {\n",
    "            'sn': sn,\n",
    "            'fault_time': fault_time,\n",
    "            'logs_count': logs_count,\n",
    "            'msg_nunique': msg_nunique,\n",
    "            'msg_category_nunique': msg_category_nunique,\n",
    "            'msg_split_0_nunique': msg_split_0_nunique,\n",
    "            'msg_split_1_nunique': msg_split_1_nunique,\n",
    "            'msg_split_2_nunique': msg_split_2_nunique,\n",
    "            'last_category': last_category,\n",
    "            'seconds_span': seconds_span,\n",
    "            'log_time_diffs_avg': log_time_diffs_avg,\n",
    "            'log_time_diffs_max': log_time_diffs_max,\n",
    "            'log_time_diffs_min': log_time_diffs_min,\n",
    "            'log_time_diffs_std': log_time_diffs_std,\n",
    "        }\n",
    "\n",
    "        for i in range(32):\n",
    "            data[f'msg_w2v_{i}'] = w2v_emb[i]\n",
    "        for i in range(16):\n",
    "            data[f'msg_tfv_{i}'] = tfv_emb[i]\n",
    "        for i in cate_map:\n",
    "            data[f'{i}_counts'] = df[f'{i}_counts'].sum()\n",
    "            \n",
    "        if data_type == 'train':\n",
    "            data['label'] = label\n",
    "        ret.append(data)\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16869it [21:42, 12.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# 大概需要半个小时\n",
    "train = make_dataset(train_data, data_type='train')\n",
    "df_train_0 = pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12260it [15:52, 12.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# 制作测试集的特征\n",
    "\n",
    "test_data['fault_time_ts'] = test_data[\"fault_time\"].values.astype(np.int64) // 10 ** 9\n",
    "\n",
    "test = make_dataset(test_data, data_type='test')\n",
    "\n",
    "df_test_0 = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16669, 97)\n",
      "(12260, 96)\n"
     ]
    }
   ],
   "source": [
    "print(df_train_0.shape)\n",
    "print(df_test_0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# srever model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(\"../tcdata/final_sel_log_dataset_b.csv\").set_index('sn')\n",
    "def func(x):\n",
    "    if type(temp.loc[x, 'server_model']) == str:  \n",
    "        return temp.loc[x, 'server_model']     \n",
    "    else:\n",
    "        return temp.loc[x, 'server_model'][0]\n",
    "\n",
    "test_servel_model = pd.read_csv(\"../tcdata/final_submit_dataset_b.csv\")\n",
    "test_servel_model['0'] = test_servel_model['sn'].apply(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_servel_model = test_servel_model.loc[:, ['sn', '0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16669, 2)\n",
      "(12260, 2)\n"
     ]
    }
   ],
   "source": [
    "train_servel_model = pd.read_csv(\"../data/preliminary_train_ServerModel.csv\")\n",
    "# test_servel_model = pd.read_csv(\"../data/preliminary_submit_dataset_b_ServerModel.csv\")\n",
    "print(train_servel_model.shape)\n",
    "print(test_servel_model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加servermodel 作为特征\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le = le.fit(train_servel_model[\"0\"].unique())\n",
    "NewTrainServerModel = np.array(le.transform(train_servel_model[\"0\"]))\n",
    "train_servel_model_all =pd.concat([train_servel_model,pd.DataFrame(NewTrainServerModel,columns=[\"sm_num\"])],axis=1)\n",
    "NewTestServerModel = np.array(le.transform(test_servel_model[\"0\"]))\n",
    "test_servel_model_all =pd.concat([test_servel_model,pd.DataFrame(NewTestServerModel,columns=[\"sm_num\"])],axis=1)\n",
    "# NewTestServerModel = np.array(le.transform(test_servel_model[\"0\"])).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_servel_model_all.drop_duplicates(subset=['sn'],inplace=True)\n",
    "test_servel_model_all.drop_duplicates(subset=['sn'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16869, 98)\n",
      "(12260, 97)\n"
     ]
    }
   ],
   "source": [
    "df_train_1 = df_train_0.merge(train_servel_model_all[['sn','sm_num']],on=\"sn\",how='inner')\n",
    "df_test_1 = df_test_0.merge(test_servel_model_all[['sn','sm_num']],on=\"sn\")\n",
    "\n",
    "print(df_train_1.shape)\n",
    "print(df_test_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_1.copy()\n",
    "df_test = df_test_1.copy()\n",
    "# df_train = df_train_0.copy()\n",
    "# df_test = df_test_0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['label1'] = df_train['label'].apply(lambda x: x-1 if x in [2, 3] else 0)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=[0, 1, 2], y=df_train['label1'])\n",
    "class_weights1 = dict(zip([0, 1, 2], weights))\n",
    "weights = compute_class_weight(class_weight='balanced', classes=[0, 1], y=df_train['label'][(df_train['label'] == 0) | (df_train['label'] == 1)])\n",
    "class_weights2 = dict(zip([0, 1], weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1(target_df, submit_df):\n",
    "    \"\"\"\n",
    "    计算得分\n",
    "    :param target_df: [sn,fault_time,label]\n",
    "    :param submit_df: [sn,fault_time,label]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    weights =  [3/7,  2/7,  1/7,  1/7]\n",
    "    macro_F1 =  0.\n",
    "    for i in  range(len(weights)):\n",
    "        TP =  sum((target_df == i) & (submit_df == i))\n",
    "        FP =  sum((target_df != i) & (submit_df == i))\n",
    "        FN =  sum((target_df == i) & (submit_df != i))\n",
    "        precision = TP /  (TP + FP)  if  (TP + FP)  >  0  else  0\n",
    "        recall = TP /  (TP + FN)  if  (TP + FN)  >  0  else  0\n",
    "        F1 =  2  * precision * recall /  (precision + recall)  if  (precision + recall)  >  0  else  0\n",
    "        macro_F1 += weights[i]  * F1\n",
    "        print(\"类别\"+str(i)+\"    precision:\"+str(precision)+\"     recall:\"+str(recall)+\"         F1:\"+str(F1))\n",
    "    return macro_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_catboost(class_weights, cat_features, cls_kind, NUM_CLASSES=None):\n",
    "    params = { \n",
    "            'task_type': 'CPU', \n",
    "            'bootstrap_type': 'Bernoulli',\n",
    "            'learning_rate': 0.05, \n",
    "            'eval_metric': cls_kind, \n",
    "            'loss_function': cls_kind, \n",
    "            'classes_count': NUM_CLASSES, \n",
    "            'iterations': 2000, # 生成多少颗对称树\n",
    "            'random_seed': 2022, \n",
    "            'depth': 8, \n",
    "            'subsample': 0.8, \n",
    "            'leaf_estimation_iterations': 8,\n",
    "            'reg_lambda': 0.5,\n",
    "            'class_weights': class_weights,\n",
    "            'early_stopping_rounds': 100 ,\n",
    "            'cat_features':cat_features,  # 设置无序离散型特征\n",
    "            'one_hot_max_size':5   # 设置one-hot最大编码\n",
    "        }\n",
    "    model = CatBoostClassifier(**params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_xgb(class_weights, cat_features, cls_kind, NUM_CLASSES=None):\n",
    "    params = { \n",
    "            'task_type': 'CPU', \n",
    "            'bootstrap_type': 'Bernoulli',\n",
    "            'learning_rate': 0.05, \n",
    "            'classes_count': NUM_CLASSES, \n",
    "            'iterations': 2000, # 生成多少颗对称树\n",
    "            'random_seed': 2022, \n",
    "            'depth': 8, \n",
    "            'subsample': 0.8, \n",
    "            'leaf_estimation_iterations': 8,\n",
    "            'reg_lambda': 0.5,\n",
    "            'class_weights': class_weights,\n",
    "            'early_stopping_rounds': 100 ,\n",
    "            'cat_features':cat_features,  # 设置无序离散型特征\n",
    "            'one_hot_max_size':5   # 设置one-hot最大编码\n",
    "        }\n",
    "    model = XGBClassifier(**params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_lgb(class_weights, cat_features, cls_kind, NUM_CLASSES=None):\n",
    "    model = LGBMClassifier(\n",
    "    # **params\n",
    "        class_weights = class_weights,\n",
    "        classes_count = NUM_CLASSES, \n",
    "        cat_features = cat_features,\n",
    "        loss_function = cls_kind, \n",
    "        random_state = 626, \n",
    "        n_estimators = 800,\n",
    "        learning_rate = 0.1,\n",
    "        max_depth = -1,\n",
    "        num_leaves = 127,\n",
    "        colsample_bytree = 0.8, \n",
    "        subsample = 0.8,\n",
    "        lambda_l1 = 0.1,   # 0.1\n",
    "        lambda_l2 = 0.2,  # 0.2\n",
    "        device='cpu'\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 10\n",
    "TARGET = 'label'\n",
    "TARGET2 = 'label1'\n",
    "a = ['sn', 'fault_time', TARGET, TARGET2,\n",
    "    'Physical_counts',\n",
    "    'Reserved_counts',\n",
    "    'Cable/Interconnect_counts',\n",
    "    'Battery_counts',\n",
    "    'Slot/Connector_counts',\n",
    "    'Fan_counts',\n",
    "    'LAN_counts',\n",
    "    'Add-in_counts',\n",
    "    'Session_counts',\n",
    "    'Chip_counts',\n",
    "    'Cable_counts',\n",
    "    'reserved_counts',\n",
    "    'Version_counts',\n",
    "    'Terminator_counts',\n",
    "    'Slot_counts',\n",
    "    'Drive_counts',\n",
    "    'Watchdog2_counts',\n",
    "    'device_counts',\n",
    "    'Event_counts',\n",
    "    'Management_counts',\n",
    "    'OEM_counts',\n",
    "    'Microcontroller/Coprocessor_counts',\n",
    "    'Chassis_counts']\n",
    "\n",
    "# a = ['sn', 'fault_time', TARGET, TARGET2]\n",
    "use_features = [col for col in df_train.columns if col not in a ]\n",
    "\n",
    "cat_features = [use_features.index(\"sm_num\"), use_features.index(\"last_category\")]  # 分类特征\n",
    "model1_useless, model2_useless = {i:0 for i in df_train.columns}, {i:0 for i in df_train.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "0:\tlearn: 0.9886388\ttest: 0.9893206\tbest: 0.9893206 (0)\ttotal: 134ms\tremaining: 4m 27s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.2426227979\n",
      "bestIteration = 171\n",
      "\n",
      "Shrink model to first 172 iterations.\n",
      "0:\tlearn: 0.6876646\ttest: 0.6891104\tbest: 0.6891104 (0)\ttotal: 122ms\tremaining: 4m 4s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.65178558\n",
      "bestIteration = 20\n",
      "\n",
      "Shrink model to first 21 iterations.\n",
      "类别0    precision:0.32075471698113206     recall:0.34         F1:0.3300970873786408\n",
      "类别1    precision:0.6927536231884058     recall:0.6967930029154519         F1:0.694767441860465\n",
      "类别2    precision:0.9546925566343042     recall:0.913312693498452         F1:0.9335443037974683\n",
      "类别3    precision:0.83203125     recall:0.9466666666666667         F1:0.8856548856548856\n",
      "F1 score: 0.5998607621870294\n",
      "Fold 2\n",
      "0:\tlearn: 0.9921688\ttest: 0.9909137\tbest: 0.9909137 (0)\ttotal: 182ms\tremaining: 6m 3s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.2182065678\n",
      "bestIteration = 239\n",
      "\n",
      "Shrink model to first 240 iterations.\n",
      "0:\tlearn: 0.6856935\ttest: 0.6882384\tbest: 0.6882384 (0)\ttotal: 81.5ms\tremaining: 2m 42s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6195381836\n",
      "bestIteration = 71\n",
      "\n",
      "Shrink model to first 72 iterations.\n",
      "类别0    precision:0.38071065989847713     recall:0.4966887417218543         F1:0.43103448275862066\n",
      "类别1    precision:0.7210031347962382     recall:0.6353591160220995         F1:0.6754772393538914\n",
      "类别2    precision:0.9540481400437637     recall:0.9316239316239316         F1:0.9427027027027027\n",
      "类别3    precision:0.8793774319066148     recall:0.9495798319327731         F1:0.9131313131313131\n",
      "F1 score: 0.6428417061168087\n",
      "Fold 3\n",
      "0:\tlearn: 0.9861641\ttest: 0.9891678\tbest: 0.9891678 (0)\ttotal: 110ms\tremaining: 3m 39s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.268068887\n",
      "bestIteration = 205\n",
      "\n",
      "Shrink model to first 206 iterations.\n",
      "0:\tlearn: 0.6887498\ttest: 0.6894047\tbest: 0.6894047 (0)\ttotal: 54.7ms\tremaining: 1m 49s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6315322792\n",
      "bestIteration = 65\n",
      "\n",
      "Shrink model to first 66 iterations.\n",
      "类别0    precision:0.38285714285714284     recall:0.4036144578313253         F1:0.3929618768328445\n",
      "类别1    precision:0.7     recall:0.6885245901639344         F1:0.6942148760330578\n",
      "类别2    precision:0.9448732083792724     recall:0.9155982905982906         F1:0.930005425935974\n",
      "类别3    precision:0.8204081632653061     recall:0.9178082191780822         F1:0.8663793103448275\n",
      "F1 score: 0.6233857312636357\n",
      "Fold 4\n",
      "0:\tlearn: 0.9918937\ttest: 0.9909382\tbest: 0.9909382 (0)\ttotal: 111ms\tremaining: 3m 42s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.2525654344\n",
      "bestIteration = 175\n",
      "\n",
      "Shrink model to first 176 iterations.\n",
      "0:\tlearn: 0.6860191\ttest: 0.6872032\tbest: 0.6872032 (0)\ttotal: 61ms\tremaining: 2m 1s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.5885213291\n",
      "bestIteration = 172\n",
      "\n",
      "Shrink model to first 173 iterations.\n",
      "类别0    precision:0.46116504854368934     recall:0.5428571428571428         F1:0.4986876640419948\n",
      "类别1    precision:0.6965317919075145     recall:0.6639118457300276         F1:0.6798307475317349\n",
      "类别2    precision:0.9439359267734554     recall:0.9187082405345212         F1:0.9311512415349887\n",
      "类别3    precision:0.8812260536398467     recall:0.9163346613545816         F1:0.8984375\n",
      "F1 score: 0.669330461246349\n",
      "Fold 5\n",
      "0:\tlearn: 0.9879680\ttest: 0.9896788\tbest: 0.9896788 (0)\ttotal: 110ms\tremaining: 3m 40s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.2592442556\n",
      "bestIteration = 176\n",
      "\n",
      "Shrink model to first 177 iterations.\n",
      "0:\tlearn: 0.6874530\ttest: 0.6887941\tbest: 0.6887941 (0)\ttotal: 90.9ms\tremaining: 3m 1s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6296067014\n",
      "bestIteration = 78\n",
      "\n",
      "Shrink model to first 79 iterations.\n",
      "类别0    precision:0.3333333333333333     recall:0.43243243243243246         F1:0.3764705882352941\n",
      "类别1    precision:0.7039274924471299     recall:0.6526610644257703         F1:0.6773255813953489\n",
      "类别2    precision:0.9497267759562842     recall:0.9264392324093816         F1:0.9379384781435509\n",
      "类别3    precision:0.8835341365461847     recall:0.9016393442622951         F1:0.8924949290060852\n",
      "F1 score: 0.6163566192351737\n",
      "Fold 6\n",
      "0:\tlearn: 0.9884217\ttest: 0.9867341\tbest: 0.9867341 (0)\ttotal: 113ms\tremaining: 3m 45s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.2358234824\n",
      "bestIteration = 241\n",
      "\n",
      "Shrink model to first 242 iterations.\n",
      "0:\tlearn: 0.6856219\ttest: 0.6874112\tbest: 0.6874112 (0)\ttotal: 74.9ms\tremaining: 2m 29s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.63199817\n",
      "bestIteration = 40\n",
      "\n",
      "Shrink model to first 41 iterations.\n",
      "类别0    precision:0.3837837837837838     recall:0.4382716049382716         F1:0.4092219020172911\n",
      "类别1    precision:0.7232142857142857     recall:0.6962750716332379         F1:0.7094890510948905\n",
      "类别2    precision:0.9532608695652174     recall:0.9349680170575693         F1:0.9440258342303552\n",
      "类别3    precision:0.8780487804878049     recall:0.907563025210084         F1:0.8925619834710744\n",
      "F1 score: 0.6404616608490119\n",
      "Fold 7\n",
      "0:\tlearn: 0.9864554\ttest: 0.9883853\tbest: 0.9883853 (0)\ttotal: 113ms\tremaining: 3m 45s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.3133747908\n",
      "bestIteration = 115\n",
      "\n",
      "Shrink model to first 116 iterations.\n",
      "0:\tlearn: 0.6868738\ttest: 0.6870710\tbest: 0.6870710 (0)\ttotal: 110ms\tremaining: 3m 40s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6326035981\n",
      "bestIteration = 56\n",
      "\n",
      "Shrink model to first 57 iterations.\n",
      "类别0    precision:0.4143646408839779     recall:0.4437869822485207         F1:0.42857142857142855\n",
      "类别1    precision:0.6901408450704225     recall:0.716374269005848         F1:0.7030129124820661\n",
      "类别2    precision:0.9470720720720721     recall:0.9191256830601093         F1:0.932889628397116\n",
      "类别3    precision:0.8593155893536122     recall:0.8659003831417624         F1:0.8625954198473283\n",
      "F1 score: 0.6410321655604088\n",
      "Fold 8\n",
      "0:\tlearn: 0.9908059\ttest: 0.9940352\tbest: 0.9940352 (0)\ttotal: 112ms\tremaining: 3m 43s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.3005919844\n",
      "bestIteration = 96\n",
      "\n",
      "Shrink model to first 97 iterations.\n",
      "0:\tlearn: 0.6871438\ttest: 0.6883662\tbest: 0.6883662 (0)\ttotal: 86ms\tremaining: 2m 51s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6420785219\n",
      "bestIteration = 30\n",
      "\n",
      "Shrink model to first 31 iterations.\n",
      "类别0    precision:0.37158469945355194     recall:0.44155844155844154         F1:0.4035608308605341\n",
      "类别1    precision:0.6624605678233438     recall:0.658307210031348         F1:0.660377358490566\n",
      "类别2    precision:0.9472527472527472     recall:0.9141039236479321         F1:0.9303831624392878\n",
      "类别3    precision:0.8700361010830325     recall:0.8892988929889298         F1:0.8795620437956203\n",
      "F1 score: 0.6201974879710918\n",
      "Fold 9\n",
      "0:\tlearn: 0.9886318\ttest: 0.9874202\tbest: 0.9874202 (0)\ttotal: 121ms\tremaining: 4m 1s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.2596819078\n",
      "bestIteration = 153\n",
      "\n",
      "Shrink model to first 154 iterations.\n",
      "0:\tlearn: 0.6857515\ttest: 0.6890110\tbest: 0.6890110 (0)\ttotal: 59.3ms\tremaining: 1m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6329362206\n",
      "bestIteration = 63\n",
      "\n",
      "Shrink model to first 64 iterations.\n",
      "类别0    precision:0.357487922705314     recall:0.4378698224852071         F1:0.3936170212765957\n",
      "类别1    precision:0.6885245901639344     recall:0.6287425149700598         F1:0.6572769953051644\n",
      "类别2    precision:0.9436619718309859     recall:0.9216931216931217         F1:0.9325481798715203\n",
      "类别3    precision:0.873015873015873     recall:0.9205020920502092         F1:0.8961303462321792\n",
      "F1 score: 0.6177262257919736\n",
      "Fold 10\n",
      "0:\tlearn: 0.9911862\ttest: 0.9930250\tbest: 0.9930250 (0)\ttotal: 120ms\tremaining: 3m 59s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.245541208\n",
      "bestIteration = 188\n",
      "\n",
      "Shrink model to first 189 iterations.\n",
      "0:\tlearn: 0.6858900\ttest: 0.6877756\tbest: 0.6877756 (0)\ttotal: 59.1ms\tremaining: 1m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6191275877\n",
      "bestIteration = 51\n",
      "\n",
      "Shrink model to first 52 iterations.\n",
      "类别0    precision:0.42790697674418604     recall:0.5168539325842697         F1:0.4681933842239186\n",
      "类别1    precision:0.7223880597014926     recall:0.6522911051212938         F1:0.6855524079320113\n",
      "类别2    precision:0.9506172839506173     recall:0.9390243902439024         F1:0.9447852760736196\n",
      "类别3    precision:0.8653061224489796     recall:0.902127659574468         F1:0.8833333333333333\n",
      "F1 score: 0.6576862254203901\n",
      "类别0    precision:0.38526315789473686     recall:0.45129469790382243         F1:0.41567291311754684\n",
      "类别1    precision:0.7002090176171991     recall:0.6688533941814033         F1:0.6841721371261853\n",
      "类别2    precision:0.948946962178851     recall:0.9233905579399142         F1:0.9359943444450487\n",
      "类别3    precision:0.8643669149353195     recall:0.9107806691449815         F1:0.8869670152855993\n",
      "catboost 0.6340463390479513\n",
      "Fold 1\n",
      "[20:22:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"classes_count\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:22:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.04013\n",
      "[99]\tvalidation_0-mlogloss:0.22552\n",
      "[20:22:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:22:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68339\n",
      "[99]\tvalidation_0-logloss:0.58747\n",
      "类别0    precision:0.5     recall:0.18         F1:0.2647058823529412\n",
      "类别1    precision:0.6761904761904762     recall:0.8279883381924198         F1:0.7444298820445608\n",
      "类别2    precision:0.9392378990731205     recall:0.9411764705882353         F1:0.9402061855670103\n",
      "类别3    precision:0.8512396694214877     recall:0.9155555555555556         F1:0.8822269807280514\n",
      "F1 score: 0.586487225349001\n",
      "Fold 2\n",
      "[20:22:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"classes_count\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:22:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.04029\n",
      "[99]\tvalidation_0-mlogloss:0.20799\n",
      "[20:23:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68323\n",
      "[99]\tvalidation_0-logloss:0.56406\n",
      "类别0    precision:0.5178571428571429     recall:0.19205298013245034         F1:0.28019323671497587\n",
      "类别1    precision:0.6817102137767221     recall:0.7928176795580111         F1:0.7330779054916986\n",
      "类别2    precision:0.9354166666666667     recall:0.9594017094017094         F1:0.9472573839662448\n",
      "类别3    precision:0.884     recall:0.9285714285714286         F1:0.9057377049180327\n",
      "F1 score: 0.5942472300018002\n",
      "Fold 3\n",
      "[20:23:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"classes_count\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.04074\n",
      "[99]\tvalidation_0-mlogloss:0.25071\n",
      "[20:23:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68483\n",
      "[99]\tvalidation_0-logloss:0.57757\n",
      "类别0    precision:0.6744186046511628     recall:0.1746987951807229         F1:0.27751196172248804\n",
      "类别1    precision:0.6717724288840262     recall:0.8387978142076503         F1:0.7460510328068044\n",
      "类别2    precision:0.920997920997921     recall:0.9465811965811965         F1:0.9336143308746048\n",
      "类别3    precision:0.8533333333333334     recall:0.8767123287671232         F1:0.8648648648648648\n",
      "F1 score: 0.5890167352172203\n",
      "Fold 4\n",
      "[20:23:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"classes_count\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.04134\n",
      "[99]\tvalidation_0-mlogloss:0.23160\n",
      "[20:23:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68233\n",
      "[99]\tvalidation_0-logloss:0.54857\n",
      "类别0    precision:0.6333333333333333     recall:0.21714285714285714         F1:0.32340425531914896\n",
      "类别1    precision:0.6629955947136564     recall:0.8292011019283747         F1:0.7368421052631581\n",
      "类别2    precision:0.9283387622149837     recall:0.9521158129175946         F1:0.9400769653655854\n",
      "类别3    precision:0.8849206349206349     recall:0.8884462151394422         F1:0.8866799204771372\n",
      "F1 score: 0.6100934089037836\n",
      "Fold 5\n",
      "[20:23:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"classes_count\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.04008\n",
      "[99]\tvalidation_0-mlogloss:0.22634\n",
      "[20:23:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68265\n",
      "[99]\tvalidation_0-logloss:0.55234\n",
      "类别0    precision:0.5087719298245614     recall:0.19594594594594594         F1:0.2829268292682927\n",
      "类别1    precision:0.6966824644549763     recall:0.8235294117647058         F1:0.7548138639281129\n",
      "类别2    precision:0.9307135470527405     recall:0.9594882729211087         F1:0.9448818897637795\n",
      "类别3    precision:0.8962655601659751     recall:0.8852459016393442         F1:0.8907216494845361\n",
      "F1 score: 0.5991445364156313\n",
      "Fold 6\n",
      "[20:23:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"classes_count\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.03962\n",
      "[99]\tvalidation_0-mlogloss:0.21285\n",
      "[20:23:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68159\n",
      "[99]\tvalidation_0-logloss:0.57657\n",
      "类别0    precision:0.5217391304347826     recall:0.2222222222222222         F1:0.3116883116883117\n",
      "类别1    precision:0.6842105263157895     recall:0.8194842406876791         F1:0.7457627118644068\n",
      "类别2    precision:0.9364583333333333     recall:0.9584221748400853         F1:0.9473129610115911\n",
      "类别3    precision:0.8916666666666667     recall:0.8991596638655462         F1:0.895397489539749\n",
      "F1 score: 0.609900115620727\n",
      "Fold 7\n",
      "[20:23:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"classes_count\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.04109\n",
      "[99]\tvalidation_0-mlogloss:0.25494\n",
      "[20:23:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68520\n",
      "[99]\tvalidation_0-logloss:0.59709\n",
      "类别0    precision:0.48484848484848486     recall:0.1893491124260355         F1:0.2723404255319149\n",
      "类别1    precision:0.6405529953917051     recall:0.8128654970760234         F1:0.7164948453608248\n",
      "类别2    precision:0.9260450160771704     recall:0.9442622950819672         F1:0.9350649350649352\n",
      "类别3    precision:0.8740157480314961     recall:0.8505747126436781         F1:0.8621359223300972\n",
      "F1 score: 0.578173117816061\n",
      "Fold 8\n",
      "[20:23:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"classes_count\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.04151\n",
      "[99]\tvalidation_0-mlogloss:0.25665\n",
      "[20:23:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68396\n",
      "[99]\tvalidation_0-logloss:0.59171\n",
      "类别0    precision:0.5510204081632653     recall:0.17532467532467533         F1:0.2660098522167488\n",
      "类别1    precision:0.6256038647342995     recall:0.8119122257053292         F1:0.7066848567530697\n",
      "类别2    precision:0.9341692789968652     recall:0.9480381760339343         F1:0.9410526315789474\n",
      "类别3    precision:0.8838951310861424     recall:0.8708487084870848         F1:0.8773234200743495\n",
      "F1 score: 0.5756821888299546\n",
      "Fold 9\n",
      "[20:23:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"classes_count\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.04035\n",
      "[99]\tvalidation_0-mlogloss:0.23794\n",
      "[20:23:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68366\n",
      "[99]\tvalidation_0-logloss:0.57977\n",
      "类别0    precision:0.5166666666666667     recall:0.1834319526627219         F1:0.27074235807860264\n",
      "类别1    precision:0.6458333333333334     recall:0.8353293413173652         F1:0.7284595300261096\n",
      "类别2    precision:0.9311783107403545     recall:0.944973544973545         F1:0.9380252100840337\n",
      "类别3    precision:0.9110169491525424     recall:0.899581589958159         F1:0.9052631578947369\n",
      "F1 score: 0.5874906431809711\n",
      "Fold 10\n",
      "[20:23:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"classes_count\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.04004\n",
      "[99]\tvalidation_0-mlogloss:0.20795\n",
      "[20:23:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"bootstrap_type\", \"cat_features\", \"class_weights\", \"depth\", \"early_stopping_rounds\", \"iterations\", \"leaf_estimation_iterations\", \"one_hot_max_size\", \"random_seed\", \"task_type\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:23:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68220\n",
      "[99]\tvalidation_0-logloss:0.57821\n",
      "类别0    precision:0.6071428571428571     recall:0.19101123595505617         F1:0.29059829059829057\n",
      "类别1    precision:0.6914660831509847     recall:0.8517520215633423         F1:0.7632850241545893\n",
      "类别2    precision:0.9307036247334755     recall:0.967849223946785         F1:0.948913043478261\n",
      "类别3    precision:0.902127659574468     recall:0.902127659574468         F1:0.902127659574468\n",
      "F1 score: 0.6070579461652541\n",
      "类别0    precision:0.5473684210526316     recall:0.19235511713933415         F1:0.2846715328467153\n",
      "类别1    precision:0.6678216678216679     recall:0.8245864232743868         F1:0.7379706445437141\n",
      "类别2    precision:0.931360201511335     recall:0.9521459227467811         F1:0.9416383701188454\n",
      "类别3    precision:0.8832923832923832     recall:0.8909541511771994         F1:0.8871067242442937\n",
      "xgb 0.5941001402843876\n",
      "Fold 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unknown type of parameter:class_weights, got:dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CHENXI~1\\AppData\\Local\\Temp/ipykernel_1068/1245218832.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m               verbose=1000000)  \n\u001b[0;32m     19\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             model1.fit(x_train.values, \n\u001b[0m\u001b[0;32m     21\u001b[0m               \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m               \u001b[0meval_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python\\anaconda3\\envs\\pytorch-cuda111\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    965\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m         super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n\u001b[0m\u001b[0;32m    968\u001b[0m                     \u001b[0meval_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_sample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m                     \u001b[0meval_class_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_class_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_init_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python\\anaconda3\\envs\\pytorch-cuda111\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m         self._Booster = train(\n\u001b[0m\u001b[0;32m    749\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python\\anaconda3\\envs\\pytorch-cuda111\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;31m# construct booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python\\anaconda3\\envs\\pytorch-cuda111\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   2603\u001b[0m                 )\n\u001b[0;32m   2604\u001b[0m             \u001b[1;31m# construct booster object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2605\u001b[1;33m             \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2606\u001b[0m             \u001b[1;31m# copy the parameters from train_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2607\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python\\anaconda3\\envs\\pytorch-cuda111\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1813\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m                 \u001b[1;31m# create train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1815\u001b[1;33m                 self._lazy_init(self.data, label=self.label,\n\u001b[0m\u001b[0;32m   1816\u001b[0m                                 \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python\\anaconda3\\envs\\pytorch-cuda111\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m   1515\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'categorical_column'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategorical_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1517\u001b[1;33m         \u001b[0mparams_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_dict_to_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1518\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m         \u001b[1;31m# process for reference dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Python\\anaconda3\\envs\\pytorch-cuda111\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mparam_dict_to_str\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0mpairs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{key}={val}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Unknown type of parameter:{key}, got:{type(val).__name__}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unknown type of parameter:class_weights, got:dict"
     ]
    }
   ],
   "source": [
    "for model_name, model_builder in zip(['catboost', 'xgb', 'lgb'], [creat_catboost, creat_xgb, creat_lgb]):\n",
    "    y_pred = np.zeros((len(df_test), 4))\n",
    "    y_pred_res = np.zeros((len(df_test), 4))\n",
    "    folds = GroupKFold(n_splits=FOLDS)\n",
    "    oof_pred = np.zeros(len(df_train))\n",
    "    for fold, (tr_ind, val_ind) in enumerate(folds.split(df_train, df_train[TARGET], df_train['sn'])):\n",
    "        print(f'Fold {fold + 1}') \n",
    "        # model1: three classification\n",
    "        target = 'label1'\n",
    "        NUM_CLASSES = df_train[target].nunique()\n",
    "        x_train, x_val = df_train[use_features].iloc[tr_ind], df_train[use_features].iloc[val_ind] \n",
    "        y_train, y_val = df_train[target].iloc[tr_ind], df_train[target].iloc[val_ind]\n",
    "        model1 = model_builder(class_weights1, cat_features, 'MultiClass', NUM_CLASSES)  \n",
    "        if model_name == 'catboost':\n",
    "            model1.fit(x_train, \n",
    "              y_train, \n",
    "              eval_set = (x_val, y_val),\n",
    "              verbose=1000000)  \n",
    "        else:\n",
    "            model1.fit(x_train.values, \n",
    "              y_train.values, \n",
    "              eval_set = [(x_val.values, y_val.values)],\n",
    "              verbose=1000000)    \n",
    "        feat_imp = pd.DataFrame({'imp': model1.feature_importances_, 'feature': use_features})\n",
    "        feat_imp = feat_imp.sort_values(by='imp').reset_index(drop=True)\n",
    "        for i in feat_imp.index:\n",
    "            if feat_imp.loc[i, 'imp'] < 0.1:\n",
    "                model1_useless[feat_imp.loc[i, 'feature']] += 1\n",
    "\n",
    "        # model2: binary classification\n",
    "        target = 'label'\n",
    "        x_train, x_val = df_train[use_features].iloc[tr_ind], df_train[use_features].iloc[val_ind] \n",
    "        y_train, y_val = df_train[target].iloc[tr_ind], df_train[target].iloc[val_ind]\n",
    "        x_train_2 = x_train[(y_train == 0) | (y_train == 1)]\n",
    "        x_val_2 = x_val[(y_val == 0) | (y_val == 1)]\n",
    "        y_train_2 = y_train[(y_train == 0) | (y_train == 1)]\n",
    "        y_val_2 = y_val[(y_val == 0) | (y_val == 1)]\n",
    "        NUM_CLASSES = 2\n",
    "        model2 = model_builder(class_weights2, cat_features, 'Logloss')  \n",
    "        if model_name == 'catboost':\n",
    "            model2.fit(x_train_2, \n",
    "                      y_train_2, \n",
    "                      eval_set=(x_val_2, y_val_2), \n",
    "                      verbose=1000000)\n",
    "        else:\n",
    "            model2.fit(x_train_2.values, \n",
    "                      y_train_2.values, \n",
    "                      eval_set=[(x_val_2.values, y_val_2.values)], \n",
    "                      verbose=1000000)\n",
    "\n",
    "        predicted = softmax(model1.predict_proba(x_val)).argmax(axis=1) + 1\n",
    "        predicted[predicted == 1] = softmax(model2.predict_proba(x_val[predicted == 1])).argmax(axis=1)\n",
    "        oof_pred[val_ind] = predicted # 输出概率\n",
    "        score = macro_f1(df_train['label'].iloc[val_ind], predicted)\n",
    "        # 测试集，\n",
    "        y_pred1 = softmax(model1.predict_proba(df_test[use_features]))\n",
    "        y_pred[(y_pred1.argmax(axis=1) != 0), 0:2] = 0\n",
    "        y_pred[(y_pred1.argmax(axis=1) != 0), 2:] = y_pred1[(y_pred1.argmax(axis=1) != 0)][:, 1:] / y_pred1[(y_pred1.argmax(axis=1) != 0)][:, 1:].sum(axis=1).reshape(-1, 1)\n",
    "        y_pred[(y_pred1.argmax(axis=1) == 0), 2:] = 0\n",
    "        y_pred[(y_pred1.argmax(axis=1) == 0), 0:2] = softmax(model2.predict_proba(df_test[use_features][y_pred1.argmax(axis=1) == 0]))\n",
    "        y_pred_res +=  y_pred / folds.n_splits\n",
    "\n",
    "        # score = macro_f1(y_val, oof_pred[val_ind].argmax(axis=1), average='macro')\n",
    "        print(f'F1 score: {score}')       \n",
    "        feat_imp = pd.DataFrame({'imp': model2.feature_importances_, 'feature': use_features})\n",
    "        feat_imp = feat_imp.sort_values(by='imp').reset_index(drop=True)\n",
    "        for i in feat_imp.index:\n",
    "            if feat_imp.loc[i, 'imp'] < 0.1:\n",
    "                model2_useless[feat_imp.loc[i, 'feature']] += 1\n",
    "\n",
    "    #     print(\"Features importance...\")\n",
    "    #     \n",
    "\n",
    "        del x_train, x_val, y_train, y_val, x_train_2, x_val_2, y_train_2, y_val_2,\n",
    "        gc.collect()\n",
    "    print(model_name, macro_f1(df_train['label'], oof_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = creat_xgb(class_weights1, cat_features, 'MultiClass', NUM_CLASSES)  \n",
    "\n",
    "model1.fit(x_train.values, \n",
    "          y_train.values, \n",
    "          eval_set = [(x_val.values, y_val.values)],\n",
    "          verbose=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (valid_X, valid_y) in enumerate(eval_set):\n",
    "    print(valid_X, valid_y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = df_test[['sn', 'fault_time']].copy()\n",
    "sub['label'] = y_pred_res.argmax(axis=1)\n",
    "# display(sub.head())\n",
    "sub['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "sub.to_csv(f'predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in model1_useless:\n",
    "    if model1_useless[i] >= 7:\n",
    "        a.append(i)\n",
    "b = []\n",
    "for i in model2_useless:\n",
    "    if model2_useless[i] >= 7:\n",
    "        b.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.extend(['sn', 'fault_time', TARGET, TARGET2])\n",
    "use_features1 = [col for col in df_train.columns if col not in a ]\n",
    "b.extend(['sn', 'fault_time', TARGET, TARGET2])\n",
    "use_features2 = [col for col in df_train.columns if col not in b ]\n",
    "cat_features1 = [use_features1.index(\"sm_num\"), use_features1.index(\"last_category\")]  # 分类特征\n",
    "cat_features2 = [use_features2.index(\"sm_num\")]  # 分类特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros((len(df_test), 4))\n",
    "folds = GroupKFold(n_splits=FOLDS)\n",
    "oof_pred = np.zeros(len(df_train))\n",
    "for fold, (tr_ind, val_ind) in enumerate(folds.split(df_train, df_train[TARGET], df_train['sn'])):\n",
    "    print(f'Fold {fold + 1}') \n",
    "    # model1: three classification\n",
    "    target = 'label1'\n",
    "    NUM_CLASSES = df_train[target].nunique()\n",
    "    x_train_1, x_val_1 = df_train[use_features1].iloc[tr_ind], df_train[use_features1].iloc[val_ind] \n",
    "    y_train_1, y_val_1 = df_train[target].iloc[tr_ind], df_train[target].iloc[val_ind]\n",
    "    model1 = creat_model(class_weights1, cat_features1, 'MultiClass', NUM_CLASSES)  \n",
    "    model1.fit(x_train_1, \n",
    "              y_train_1, \n",
    "              eval_set=(x_val_1, y_val_1), \n",
    "              verbose=100)    \n",
    "\n",
    "    # model2: binary classification\n",
    "    target = 'label'\n",
    "    x_train, x_val = df_train[use_features2].iloc[tr_ind], df_train[use_features2].iloc[val_ind] \n",
    "    y_train, y_val = df_train[target].iloc[tr_ind], df_train[target].iloc[val_ind]\n",
    "    x_train_2 = x_train[(y_train == 0) | (y_train == 1)]\n",
    "    x_val_2 = x_val[(y_val == 0) | (y_val == 1)]\n",
    "    y_train_2 = y_train[(y_train == 0) | (y_train == 1)]\n",
    "    y_val_2 = y_val[(y_val == 0) | (y_val == 1)]\n",
    "    NUM_CLASSES = 2\n",
    "    model2 = creat_model(class_weights2, cat_features2, 'Logloss')  \n",
    "    model2.fit(x_train_2, \n",
    "              y_train_2, \n",
    "              eval_set=(x_val_2, y_val_2), \n",
    "              verbose=100)\n",
    "\n",
    "    predicted = softmax(model1.predict_proba(x_val_1)).argmax(axis=1) + 1\n",
    "    predicted[predicted == 1] = softmax(model2.predict_proba(df_train[use_features2].iloc[val_ind][predicted == 1])).argmax(axis=1)\n",
    "    oof_pred[val_ind] = predicted # 输出概率\n",
    "    score = macro_f1(df_train['label'].iloc[val_ind], predicted)\n",
    "    # 测试集，\n",
    "    y_pred1 = softmax(model1.predict_proba(df_test[use_features1]))\n",
    "    y_pred[(y_pred1.argmax(axis=1) != 0), 0:2] = 0\n",
    "    y_pred[(y_pred1.argmax(axis=1) != 0), 2:] = y_pred1[(y_pred1.argmax(axis=1) != 0)][:, 1:] / y_pred1[(y_pred1.argmax(axis=1) != 0)][:, 1:].sum(axis=1).reshape(-1, 1)\n",
    "    y_pred[(y_pred1.argmax(axis=1) == 0), 2:] = 0\n",
    "    y_pred[(y_pred1.argmax(axis=1) == 0), 0:2] = softmax(model2.predict_proba(df_test[use_features2][y_pred1.argmax(axis=1) == 0]))\n",
    "    y_pred +=  y_pred / folds.n_splits\n",
    "\n",
    "    # score = macro_f1(y_val, oof_pred[val_ind].argmax(axis=1), average='macro')\n",
    "    print(f'F1 score: {score}')       \n",
    "#     feat_imp = pd.DataFrame({'imp': model2.feature_importances_, 'feature': use_features})\n",
    "#     feat_imp = feat_imp.sort_values(by='imp').reset_index(drop=True)\n",
    "#     for i in feat_imp.index:\n",
    "#         if feat_imp.loc[i, 'imp'] < 0.1:\n",
    "#             model2_useless[feat_imp.loc[i, 'feature']] += 1\n",
    "    \n",
    "#     print(\"Features importance...\")\n",
    "#     \n",
    "\n",
    "    del x_train, x_val, y_train, y_val, x_train_2, x_val_2, y_train_2, y_val_2,\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_f1(df_train['label'], oof_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1601\n",
       "3     625\n",
       "1     582\n",
       "0     222\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = df_test[['sn', 'fault_time']].copy()\n",
    "sub['label'] = y_pred_res.argmax(axis=1)\n",
    "# display(sub.head())\n",
    "sub['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "sub.to_csv(f'{time.strftime(\"%m-%d %H-%M-%S\", time.localtime())}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(pytorch-cuda111)",
   "language": "python",
   "name": "pytorch-cuda111"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
